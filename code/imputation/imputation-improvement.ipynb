{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9402f8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PIPELINE OTIMIZADO COM GRID SEARCH\n",
      "================================================================================\n",
      "Arquivos encontrados: 615\n",
      "Fra√ß√µes de missing: [0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
      "Pasta de resultados: ..\\..\\results\n",
      "Pasta de dados imputados: ..\\..\\datasets\\multivariada-imputed-optimized\n",
      "================================================================================\n",
      "\n",
      "\n",
      "################################################################################\n",
      "[1/615] Processando: ac-am_merged.csv\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      "FASE 1: AVALIA√á√ÉO COM GRID SEARCH\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "[AVALIANDO] ac-am_merged.csv\n",
      "================================================================================\n",
      "  [DADOS LIMPOS] 1326 amostras v√°lidas (removidos 885 com -1)\n",
      "\n",
      "  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  FRA√á√ÉO DE MISSING: 20%\n",
      "  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  [BASELINE] Avaliando fra√ß√£o 20%\n",
      "    M√°scara aplicada: 265/1326 amostras (20%)\n",
      "  [STACKING] Avaliando com test_fraction=20%\n",
      "    Cross-validation com 5 splits\n",
      "      Fold 1: treino=221, teste=221 (16.7%)\n",
      "      üîç Iniciando Grid Search nos modelos base...\n",
      "        - XGBoost...\n",
      "          ‚úì Melhores params: {'colsample_bytree': 0.7, 'learning_rate': 0.15, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "        - Random Forest...\n",
      "          ‚úì Melhores params: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 100}\n",
      "        - Gradient Boosting...\n",
      "          ‚úì Melhores params: {'learning_rate': 0.15, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "        - KNN...\n",
      "          ‚úì Melhores params: {'metric': 'minkowski', 'n_neighbors': 3, 'p': 2, 'weights': 'distance'}\n",
      "        - ElasticNet...\n",
      "          ‚úì Melhores params: {'alpha': 0.1, 'l1_ratio': 0.1, 'max_iter': 5000}\n",
      "      üîç Otimizando meta-modelo...\n",
      "        ‚úì Melhor meta-modelo: {'final_estimator': Ridge(), 'final_estimator__alpha': 1.0}\n",
      "        ‚úÖ RMSE: 63.66M, R¬≤: 0.6740, Tempo: 3.1678ms/amostra\n",
      "      Fold 2: treino=442, teste=221 (16.7%)\n",
      "      üîç Iniciando Grid Search nos modelos base...\n",
      "        - XGBoost...\n",
      "          ‚úì Melhores params: {'colsample_bytree': 0.8, 'learning_rate': 0.15, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "        - Random Forest...\n",
      "          ‚úì Melhores params: {'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 100}\n",
      "        - Gradient Boosting...\n",
      "          ‚úì Melhores params: {'learning_rate': 0.1, 'max_depth': 4, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
      "        - KNN...\n",
      "          ‚úì Melhores params: {'metric': 'minkowski', 'n_neighbors': 7, 'p': 2, 'weights': 'distance'}\n",
      "        - ElasticNet...\n",
      "          ‚úì Melhores params: {'alpha': 0.1, 'l1_ratio': 0.1, 'max_iter': 5000}\n",
      "      üîç Otimizando meta-modelo...\n",
      "        ‚úì Melhor meta-modelo: {'final_estimator': Ridge(), 'final_estimator__alpha': 0.1}\n",
      "        ‚úÖ RMSE: 746.84M, R¬≤: -50.7394, Tempo: 0.1089ms/amostra\n",
      "      Fold 3: treino=663, teste=221 (16.7%)\n",
      "      üîç Iniciando Grid Search nos modelos base...\n",
      "        - XGBoost...\n",
      "          ‚úì Melhores params: {'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.7}\n",
      "        - Random Forest...\n",
      "          ‚úì Melhores params: {'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "        - Gradient Boosting...\n",
      "          ‚úì Melhores params: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "        - KNN...\n",
      "          ‚úì Melhores params: {'metric': 'minkowski', 'n_neighbors': 9, 'p': 2, 'weights': 'distance'}\n",
      "        - ElasticNet...\n",
      "          ‚úì Melhores params: {'alpha': 0.1, 'l1_ratio': 0.1, 'max_iter': 5000}\n",
      "      üîç Otimizando meta-modelo...\n",
      "        ‚úì Melhor meta-modelo: {'final_estimator': Ridge(), 'final_estimator__alpha': 0.1}\n",
      "        ‚úÖ RMSE: 9.30M, R¬≤: 0.9946, Tempo: 0.1314ms/amostra\n",
      "      Fold 4: treino=884, teste=221 (16.7%)\n",
      "      üîç Iniciando Grid Search nos modelos base...\n",
      "        - XGBoost...\n",
      "          ‚úì Melhores params: {'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 150, 'subsample': 0.9}\n",
      "        - Random Forest...\n",
      "          ‚úì Melhores params: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 150}\n",
      "        - Gradient Boosting...\n",
      "          ‚úì Melhores params: {'learning_rate': 0.15, 'max_depth': 4, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.7}\n",
      "        - KNN...\n",
      "          ‚úì Melhores params: {'metric': 'minkowski', 'n_neighbors': 5, 'p': 2, 'weights': 'distance'}\n",
      "        - ElasticNet...\n",
      "          ‚úì Melhores params: {'alpha': 0.1, 'l1_ratio': 0.1, 'max_iter': 5000}\n",
      "      üîç Otimizando meta-modelo...\n",
      "        ‚úì Melhor meta-modelo: {'final_estimator': Ridge(), 'final_estimator__alpha': 0.1}\n",
      "        ‚úÖ RMSE: 10.26M, R¬≤: 0.9925, Tempo: 0.1040ms/amostra\n",
      "      Fold 5: treino=1105, teste=221 (16.7%)\n",
      "      üîç Iniciando Grid Search nos modelos base...\n",
      "        - XGBoost...\n",
      "          ‚úì Melhores params: {'colsample_bytree': 0.9, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.9}\n",
      "        - Random Forest...\n",
      "          ‚úì Melhores params: {'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 100}\n",
      "        - Gradient Boosting...\n",
      "          ‚úì Melhores params: {'learning_rate': 0.1, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.7}\n",
      "        - KNN...\n",
      "          ‚úì Melhores params: {'metric': 'minkowski', 'n_neighbors': 5, 'p': 2, 'weights': 'distance'}\n",
      "        - ElasticNet...\n",
      "          ‚úì Melhores params: {'alpha': 0.1, 'l1_ratio': 0.1, 'max_iter': 5000}\n",
      "      üîç Otimizando meta-modelo...\n",
      "        ‚úì Melhor meta-modelo: {'final_estimator': Ridge(), 'final_estimator__alpha': 0.1}\n",
      "        ‚úÖ RMSE: 15.22M, R¬≤: 0.9696, Tempo: 0.1745ms/amostra\n",
      "    üìä M√©dia - RMSE: 169.06M, R¬≤: -9.4217, Tempo: 0.7373ms/amostra\n",
      "\n",
      "  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  FRA√á√ÉO DE MISSING: 25%\n",
      "  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  [BASELINE] Avaliando fra√ß√£o 25%\n",
      "    M√°scara aplicada: 331/1326 amostras (25%)\n",
      "  [STACKING] Avaliando com test_fraction=25%\n",
      "    Cross-validation com 4 splits\n",
      "      Fold 1: treino=266, teste=265 (20.0%)\n",
      "      üîç Iniciando Grid Search nos modelos base...\n",
      "        - XGBoost...\n",
      "          ‚úì Melhores params: {'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 100, 'subsample': 0.7}\n",
      "        - Random Forest...\n",
      "          ‚úì Melhores params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 100}\n",
      "        - Gradient Boosting...\n",
      "          ‚úì Melhores params: {'learning_rate': 0.05, 'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.7}\n",
      "        - KNN...\n",
      "          ‚úì Melhores params: {'metric': 'minkowski', 'n_neighbors': 3, 'p': 2, 'weights': 'distance'}\n",
      "        - ElasticNet...\n",
      "          ‚úì Melhores params: {'alpha': 0.1, 'l1_ratio': 0.1, 'max_iter': 5000}\n",
      "      üîç Otimizando meta-modelo...\n",
      "        ‚úì Melhor meta-modelo: {'final_estimator': Ridge(), 'final_estimator__alpha': 1.0}\n",
      "        ‚úÖ RMSE: 3383.20M, R¬≤: -756.6832, Tempo: 0.1602ms/amostra\n",
      "      Fold 2: treino=531, teste=265 (20.0%)\n",
      "      üîç Iniciando Grid Search nos modelos base...\n",
      "        - XGBoost...\n",
      "          ‚úì Melhores params: {'colsample_bytree': 0.7, 'learning_rate': 0.15, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}\n",
      "        - Random Forest...\n",
      "          ‚úì Melhores params: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 100}\n",
      "        - Gradient Boosting...\n",
      "          ‚úì Melhores params: {'learning_rate': 0.15, 'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.7}\n",
      "        - KNN...\n",
      "          ‚úì Melhores params: {'metric': 'minkowski', 'n_neighbors': 3, 'p': 2, 'weights': 'distance'}\n",
      "        - ElasticNet...\n",
      "          ‚úì Melhores params: {'alpha': 0.1, 'l1_ratio': 0.1, 'max_iter': 5000}\n",
      "      üîç Otimizando meta-modelo...\n",
      "        ‚úì Melhor meta-modelo: {'final_estimator': Ridge(), 'final_estimator__alpha': 0.1}\n",
      "        ‚úÖ RMSE: 6.28M, R¬≤: 0.9938, Tempo: 0.1604ms/amostra\n",
      "      Fold 3: treino=796, teste=265 (20.0%)\n",
      "      üîç Iniciando Grid Search nos modelos base...\n",
      "        - XGBoost...\n",
      "          ‚úì Melhores params: {'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}\n",
      "        - Random Forest...\n",
      "          ‚úì Melhores params: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 150}\n",
      "        - Gradient Boosting...\n",
      "          ‚úì Melhores params: {'learning_rate': 0.05, 'max_depth': 4, 'min_samples_split': 5, 'n_estimators': 200, 'subsample': 0.7}\n",
      "        - KNN...\n",
      "          ‚úì Melhores params: {'metric': 'minkowski', 'n_neighbors': 3, 'p': 2, 'weights': 'distance'}\n",
      "        - ElasticNet...\n",
      "          ‚úì Melhores params: {'alpha': 0.1, 'l1_ratio': 0.1, 'max_iter': 5000}\n",
      "      üîç Otimizando meta-modelo...\n",
      "        ‚úì Melhor meta-modelo: {'final_estimator': Ridge(), 'final_estimator__alpha': 0.1}\n",
      "        ‚úÖ RMSE: 10.81M, R¬≤: 0.9907, Tempo: 0.1595ms/amostra\n",
      "      Fold 4: treino=1061, teste=265 (20.0%)\n",
      "      üîç Iniciando Grid Search nos modelos base...\n",
      "        - XGBoost...\n",
      "          ‚úì Melhores params: {'colsample_bytree': 0.9, 'learning_rate': 0.15, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.9}\n",
      "        - Random Forest...\n",
      "          ‚úì Melhores params: {'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "        - Gradient Boosting...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 961\u001b[39m\n\u001b[32m    958\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìä Sum√°rio salvo em: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m961\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 903\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    900\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFASE 1: AVALIA√á√ÉO COM GRID SEARCH\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    901\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m result = \u001b[43mevaluate_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_fractions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    906\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ‚ö†Ô∏è Arquivo ignorado (dados insuficientes)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 556\u001b[39m, in \u001b[36mevaluate_file\u001b[39m\u001b[34m(file_path, missing_fractions)\u001b[39m\n\u001b[32m    553\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m‚îÄ\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    555\u001b[39m     baseline_results = evaluate_baselines(df_clean, frac, target_col)\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m     stacking_results = \u001b[43mevaluate_stacking_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m     results[\u001b[38;5;28mstr\u001b[39m(frac)] = {\n\u001b[32m    559\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbaseline\u001b[39m\u001b[33m\"\u001b[39m: baseline_results,\n\u001b[32m    560\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstacking\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m   (...)\u001b[39m\u001b[32m    564\u001b[39m         }\n\u001b[32m    565\u001b[39m     }\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    568\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: source,\n\u001b[32m    569\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m\"\u001b[39m: results,\n\u001b[32m    570\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mn_samples\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(df_clean)\n\u001b[32m    571\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 484\u001b[39m, in \u001b[36mevaluate_stacking_cv\u001b[39m\u001b[34m(df_clean, test_fraction, target_col)\u001b[39m\n\u001b[32m    481\u001b[39m y_train_scaled = scaler_y.fit_transform(y_train.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)).ravel()\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# ‚úÖ GRID SEARCH NOS MODELOS BASE\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m base_models = \u001b[43mget_optimized_base_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;66;03m# ‚úÖ GRID SEARCH NO META-MODELO\u001b[39;00m\n\u001b[32m    487\u001b[39m stacking = get_optimized_meta_model(X_train_scaled, y_train_scaled, base_models)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 332\u001b[39m, in \u001b[36mget_optimized_base_models\u001b[39m\u001b[34m(X_train, y_train, n_jobs)\u001b[39m\n\u001b[32m    317\u001b[39m gb_params = {\n\u001b[32m    318\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mn_estimators\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m100\u001b[39m, \u001b[32m150\u001b[39m, \u001b[32m200\u001b[39m],\n\u001b[32m    319\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmax_depth\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m3\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    322\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmin_samples_split\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m5\u001b[39m]\n\u001b[32m    323\u001b[39m }\n\u001b[32m    324\u001b[39m gb_grid = GridSearchCV(\n\u001b[32m    325\u001b[39m     GradientBoostingRegressor(random_state=\u001b[32m42\u001b[39m),\n\u001b[32m    326\u001b[39m     gb_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    330\u001b[39m     verbose=\u001b[32m0\u001b[39m\n\u001b[32m    331\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m \u001b[43mgb_grid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m optimized_models.append((\u001b[33m'\u001b[39m\u001b[33mgb\u001b[39m\u001b[33m'\u001b[39m, gb_grid.best_estimator_))\n\u001b[32m    334\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m          ‚úì Melhores params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgb_grid.best_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LARCES_MALU\\Desktop\\research\\ENHANCED-IMPUTATION-PREDICTION\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LARCES_MALU\\Desktop\\research\\ENHANCED-IMPUTATION-PREDICTION\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LARCES_MALU\\Desktop\\research\\ENHANCED-IMPUTATION-PREDICTION\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LARCES_MALU\\Desktop\\research\\ENHANCED-IMPUTATION-PREDICTION\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LARCES_MALU\\Desktop\\research\\ENHANCED-IMPUTATION-PREDICTION\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LARCES_MALU\\Desktop\\research\\ENHANCED-IMPUTATION-PREDICTION\\.venv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LARCES_MALU\\Desktop\\research\\ENHANCED-IMPUTATION-PREDICTION\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LARCES_MALU\\Desktop\\research\\ENHANCED-IMPUTATION-PREDICTION\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ========== PREPARA√á√ÉO DE DADOS ==========\n",
    "\n",
    "def get_clean_data(df: pd.DataFrame, target_col: str = \"Vazao_BBR\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove apenas linhas com valores inv√°lidos (-1) no target.\n",
    "    \"\"\"\n",
    "    df_clean = df[df[target_col] != -1].copy().reset_index(drop=True)\n",
    "    print(f\"  [DADOS LIMPOS] {len(df_clean)} amostras v√°lidas (removidos {len(df) - len(df_clean)} com -1)\")\n",
    "    return df_clean\n",
    "\n",
    "def apply_random_mask(df: pd.DataFrame, missing_fraction: float, seed: int = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aplica m√°scara aleat√≥ria para BASELINES - marca valores a serem 'escondidos' para teste.\n",
    "    \"\"\"\n",
    "    df_masked = df.copy()\n",
    "    n_samples = len(df_masked)\n",
    "    n_mask = max(1, int(missing_fraction * n_samples))\n",
    "    \n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    mask_indices = np.random.choice(df_masked.index, size=n_mask, replace=False)\n",
    "    df_masked['mask_applied'] = 0\n",
    "    df_masked.loc[mask_indices, 'mask_applied'] = 1\n",
    "    \n",
    "    print(f\"    M√°scara aplicada: {n_mask}/{n_samples} amostras ({missing_fraction*100:.0f}%)\")\n",
    "    return df_masked\n",
    "\n",
    "# ========== FEATURE ENGINEERING ==========\n",
    "\n",
    "def engineer_features_for_imputation(df: pd.DataFrame, target_col: str = \"Vazao_BBR\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Feature engineering SEM data leakage para imputa√ß√£o.\n",
    "    Regras:\n",
    "    - Nunca usar o valor ATUAL do target para criar features.\n",
    "    - Substitui inf/NaN por valores num√©ricos seguros.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ‚úÖ Garantir que Data est√° como datetime e criar features temporais b√°sicas\n",
    "    if 'Data' in df.columns:\n",
    "        df['Data'] = pd.to_datetime(df['Data'])\n",
    "        df['hour'] = df['Data'].dt.hour\n",
    "        df['day_of_week'] = df['Data'].dt.dayofweek\n",
    "        df['day_of_month'] = df['Data'].dt.day\n",
    "\n",
    "    # ‚úÖ Features derivadas de outras colunas (n√£o do target)\n",
    "    df[\"Atraso_log\"] = np.log1p(df[\"Atraso(ms)\"].clip(lower=0))\n",
    "    df[\"Hop_inv\"] = 1 / (df[\"Hop_count\"] + 1)\n",
    "    df[\"Atraso_x_Hop\"] = df[\"Atraso(ms)\"] * df[\"Hop_count\"]\n",
    "    df[\"Atraso_sq\"] = df[\"Atraso(ms)\"] ** 2\n",
    "    df[\"Hop_sq\"] = df[\"Hop_count\"] ** 2\n",
    "\n",
    "    if 'hour' in df.columns:\n",
    "        df[\"Atraso_x_hour\"] = df[\"Atraso(ms)\"] * df[\"hour\"]\n",
    "        df[\"Hop_x_hour\"] = df[\"Hop_count\"] * df[\"hour\"]\n",
    "        df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour\"] / 24)\n",
    "        df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour\"] / 24)\n",
    "\n",
    "    # ‚úÖ Features do target baseadas em valores passados\n",
    "    valid_mask = df[target_col] != -1\n",
    "    target_series = df[target_col].copy()\n",
    "    target_series[~valid_mask] = np.nan\n",
    "\n",
    "    # Lags\n",
    "    for lag in [1, 2, 3, 6]:\n",
    "        df[f\"Vazao_lag{lag}\"] = target_series.shift(lag)\n",
    "\n",
    "    # Diferen√ßas e varia√ß√µes percentuais\n",
    "    df[\"Vazao_diff1\"] = target_series.diff(1)\n",
    "    df[\"Vazao_diff2\"] = target_series.diff(2)\n",
    "    df[\"Vazao_pct_change\"] = target_series.pct_change()\n",
    "\n",
    "    # Rolling features (sem vazamento)\n",
    "    for w in [3, 6]:\n",
    "        shifted = target_series.shift(1)\n",
    "        df[f\"Vazao_roll_mean_{w}\"] = shifted.rolling(window=w, min_periods=w).mean()\n",
    "        df[f\"Vazao_roll_std_{w}\"] = shifted.rolling(window=w, min_periods=w).std()\n",
    "    df[\"Vazao_roll_max_6\"] = shifted.rolling(window=6, min_periods=6).max()\n",
    "    df[\"Vazao_roll_min_6\"] = shifted.rolling(window=6, min_periods=6).min()\n",
    "\n",
    "    # Rela√ß√µes derivadas\n",
    "    lag1 = target_series.shift(1)\n",
    "    df[\"Vazao_lag1_div_Atraso\"] = lag1 / (df[\"Atraso(ms)\"] + 1)\n",
    "    df[\"Vazao_lag1_div_Hops\"] = lag1 / (df[\"Hop_count\"] + 1)\n",
    "    df[\"Efficiency_lag1\"] = lag1 / ((df[\"Atraso(ms)\"] + 1) * (df[\"Hop_count\"] + 1))\n",
    "\n",
    "    # Transforma√ß√µes seguras\n",
    "    df[\"Vazao_lag1_log\"] = np.log1p(lag1.clip(lower=0))\n",
    "    df[\"Vazao_lag1_sqrt\"] = np.sqrt(lag1.clip(lower=0))\n",
    "\n",
    "    # Estat√≠sticas de janela expandida\n",
    "    df[\"Vazao_expanding_mean\"] = target_series.shift(1).expanding(min_periods=1).mean()\n",
    "    df[\"Vazao_expanding_std\"] = target_series.shift(1).expanding(min_periods=3).std()\n",
    "\n",
    "    # Medidas globais do target\n",
    "    df['Feature_Vazao_bbr_median'] = target_series.median()\n",
    "    df['Feature_Vazao_bbr_mean'] = target_series.mean()\n",
    "\n",
    "    # Restaurar valores -1 originais\n",
    "    df.loc[~valid_mask, target_col] = -1\n",
    "\n",
    "    # ‚úÖ Tratamento final: substituir infinitos e NaNs\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    for col in df.select_dtypes(include=[np.number]).columns:\n",
    "        if col == target_col:\n",
    "            continue\n",
    "        if df[col].isna().any():\n",
    "            if any(k in col for k in ['lag', 'roll', 'diff', 'pct', 'expanding']):\n",
    "                df[col].fillna(0, inplace=True)\n",
    "            else:\n",
    "                df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# ========== C√ÅLCULO DE M√âTRICAS ==========\n",
    "\n",
    "def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray, prediction_time: float = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calcula m√©tricas de regress√£o de forma robusta.\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_pred = np.array(y_pred).flatten()\n",
    "    \n",
    "    mask = ~(np.isnan(y_true) | np.isnan(y_pred) | np.isinf(y_true) | np.isinf(y_pred))\n",
    "    \n",
    "    if mask.sum() < 2:\n",
    "        return {\"rmse\": None, \"nrmse\": None, \"r2\": None, \"mape\": None, \"prediction_time_per_sample\": None}\n",
    "    \n",
    "    y_true = y_true[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "    \n",
    "    try:\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        nrmse = (rmse / (np.mean(y_true) + 1e-8)) * 100\n",
    "        rmse_normalized = rmse / 1_000_000\n",
    "        \n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        r2 = r2 if not np.isnan(r2) and np.isfinite(r2) else None\n",
    "        \n",
    "        mape = np.mean(np.abs((y_true - y_pred) / (np.abs(y_true) + 1e-8))) * 100\n",
    "        \n",
    "        time_per_sample = None\n",
    "        if prediction_time is not None and len(y_true) > 0:\n",
    "            time_per_sample = round((prediction_time / len(y_true)) * 1000, 4)\n",
    "        \n",
    "        return {\n",
    "            \"rmse\": round(rmse_normalized, 2),\n",
    "            \"nrmse\": round(nrmse, 2),\n",
    "            \"r2\": r2,\n",
    "            \"mape\": round(mape, 2),\n",
    "            \"prediction_time_per_sample\": time_per_sample\n",
    "        }\n",
    "    except:\n",
    "        return {\"rmse\": None, \"nrmse\": None, \"r2\": None, \"mape\": None, \"prediction_time_per_sample\": None}\n",
    "\n",
    "# ========== AVALIA√á√ÉO BASELINES ==========\n",
    "\n",
    "def evaluate_baselines(df_clean: pd.DataFrame, missing_fraction: float, target_col: str = \"Vazao_BBR\") -> Dict:\n",
    "    \"\"\"\n",
    "    Avalia m√©todos baseline.\n",
    "    \"\"\"\n",
    "    print(f\"  [BASELINE] Avaliando fra√ß√£o {missing_fraction:.0%}\")\n",
    "    \n",
    "    df_masked = apply_random_mask(df_clean, missing_fraction, seed=42)\n",
    "    mask_indices = df_masked[df_masked['mask_applied'] == 1].index\n",
    "    y_true = df_masked.loc[mask_indices, target_col].values\n",
    "    \n",
    "    results = {}\n",
    "    df_with_nan = df_masked.copy()\n",
    "    df_with_nan.loc[mask_indices, target_col] = np.nan\n",
    "    \n",
    "    # M√âDIA\n",
    "    try:\n",
    "        df_mean = df_with_nan.copy()\n",
    "        mean_value = df_mean[target_col].mean()\n",
    "        df_mean[target_col] = df_mean[target_col].fillna(mean_value)\n",
    "        y_pred = df_mean.loc[mask_indices, target_col].values\n",
    "        results['Mean'] = calculate_metrics(y_true, y_pred)\n",
    "    except:\n",
    "        results['Mean'] = {\"rmse\": None, \"nrmse\": None, \"r2\": None, \"mape\": None}\n",
    "    \n",
    "    # MEDIANA\n",
    "    try:\n",
    "        df_median = df_with_nan.copy()\n",
    "        median_value = df_median[target_col].median()\n",
    "        df_median[target_col] = df_median[target_col].fillna(median_value)\n",
    "        y_pred = df_median.loc[mask_indices, target_col].values\n",
    "        results['Median'] = calculate_metrics(y_true, y_pred)\n",
    "    except:\n",
    "        results['Median'] = {\"rmse\": None, \"nrmse\": None, \"r2\": None, \"mape\": None}\n",
    "    \n",
    "    # KNN IMPUTER\n",
    "    try:\n",
    "        df_knn = df_with_nan.copy()\n",
    "        imputer = KNNImputer(n_neighbors=min(5, len(df_clean) // 2))\n",
    "        df_knn[target_col] = imputer.fit_transform(df_knn[[target_col]]).ravel()\n",
    "        y_pred = df_knn.loc[mask_indices, target_col].values\n",
    "        results['KNNImputer'] = calculate_metrics(y_true, y_pred)\n",
    "    except:\n",
    "        results['KNNImputer'] = {\"rmse\": None, \"nrmse\": None, \"r2\": None, \"mape\": None}\n",
    "    \n",
    "    # FORWARD FILL\n",
    "    try:\n",
    "        df_ffill = df_with_nan.copy()\n",
    "        df_ffill[target_col] = df_ffill[target_col].ffill().bfill()\n",
    "        y_pred = df_ffill.loc[mask_indices, target_col].values\n",
    "        results['ForwardFill'] = calculate_metrics(y_true, y_pred)\n",
    "    except:\n",
    "        results['ForwardFill'] = {\"rmse\": None, \"nrmse\": None, \"r2\": None, \"mape\": None}\n",
    "    \n",
    "    # BACKWARD FILL\n",
    "    try:\n",
    "        df_bfill = df_with_nan.copy()\n",
    "        df_bfill[target_col] = df_bfill[target_col].bfill().ffill()\n",
    "        y_pred = df_bfill.loc[mask_indices, target_col].values\n",
    "        results['BackwardFill'] = calculate_metrics(y_true, y_pred)\n",
    "    except:\n",
    "        results['BackwardFill'] = {\"rmse\": None, \"nrmse\": None, \"r2\": None, \"mape\": None}\n",
    "    \n",
    "    # ROLLING MEAN\n",
    "    try:\n",
    "        df_rolling = df_with_nan.copy()\n",
    "        df_rolling[target_col] = df_rolling[target_col].rolling(window=3, min_periods=1).mean()\n",
    "        df_rolling[target_col] = df_rolling[target_col].ffill().bfill()\n",
    "        y_pred = df_rolling.loc[mask_indices, target_col].values\n",
    "        results['RollingMean'] = calculate_metrics(y_true, y_pred)\n",
    "    except:\n",
    "        results['RollingMean'] = {\"rmse\": None, \"nrmse\": None, \"r2\": None, \"mape\": None}\n",
    "    \n",
    "    # INTERPOLA√á√ÉO LINEAR\n",
    "    try:\n",
    "        df_linear = df_with_nan.copy()\n",
    "        df_linear[target_col] = df_linear[target_col].interpolate(method='linear').ffill().bfill()\n",
    "        y_pred = df_linear.loc[mask_indices, target_col].values\n",
    "        results['LinearInterpolation'] = calculate_metrics(y_true, y_pred)\n",
    "    except:\n",
    "        results['LinearInterpolation'] = {\"rmse\": None, \"nrmse\": None, \"r2\": None, \"mape\": None}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ========== GRID SEARCH PARA MODELOS BASE ==========\n",
    "\n",
    "def get_optimized_base_models(X_train, y_train, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Retorna modelos base otimizados com Grid Search.\n",
    "    \"\"\"\n",
    "    print(f\"      üîç Iniciando Grid Search nos modelos base...\")\n",
    "    \n",
    "    # Configurar CV temporal\n",
    "    cv_splits = min(3, len(X_train) // 20)\n",
    "    tscv = TimeSeriesSplit(n_splits=cv_splits)\n",
    "    \n",
    "    optimized_models = []\n",
    "    \n",
    "    # ===== XGBoost =====\n",
    "    print(f\"        - XGBoost...\")\n",
    "    xgb_params = {\n",
    "        'n_estimators': [100, 150, 200],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'learning_rate': [0.05, 0.1, 0.15],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "    }\n",
    "    xgb_grid = GridSearchCV(\n",
    "        XGBRegressor(random_state=42, verbosity=0, n_jobs=1),\n",
    "        xgb_params,\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=0\n",
    "    )\n",
    "    xgb_grid.fit(X_train, y_train)\n",
    "    optimized_models.append(('xgb', xgb_grid.best_estimator_))\n",
    "    print(f\"          ‚úì Melhores params: {xgb_grid.best_params_}\")\n",
    "    \n",
    "    # ===== Random Forest =====\n",
    "    print(f\"        - Random Forest...\")\n",
    "    rf_params = {\n",
    "        'n_estimators': [100, 150, 200],\n",
    "        'max_depth': [10, 15, 20],\n",
    "        'min_samples_split': [2, 3, 5],\n",
    "        'min_samples_leaf': [1, 2, 3],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "    rf_grid = GridSearchCV(\n",
    "        RandomForestRegressor(random_state=42, n_jobs=1),\n",
    "        rf_params,\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=0\n",
    "    )\n",
    "    rf_grid.fit(X_train, y_train)\n",
    "    optimized_models.append(('rf', rf_grid.best_estimator_))\n",
    "    print(f\"          ‚úì Melhores params: {rf_grid.best_params_}\")\n",
    "    \n",
    "    # ===== Gradient Boosting =====\n",
    "    print(f\"        - Gradient Boosting...\")\n",
    "    gb_params = {\n",
    "        'n_estimators': [100, 150, 200],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'learning_rate': [0.05, 0.1, 0.15],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'min_samples_split': [2, 3, 5]\n",
    "    }\n",
    "    gb_grid = GridSearchCV(\n",
    "        GradientBoostingRegressor(random_state=42),\n",
    "        gb_params,\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=0\n",
    "    )\n",
    "    gb_grid.fit(X_train, y_train)\n",
    "    optimized_models.append(('gb', gb_grid.best_estimator_))\n",
    "    print(f\"          ‚úì Melhores params: {gb_grid.best_params_}\")\n",
    "    \n",
    "    # ===== KNN =====\n",
    "    print(f\"        - KNN...\")\n",
    "    knn_params = {\n",
    "        'n_neighbors': [3, 5, 7, 9],\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'p': [1, 2],\n",
    "        'metric': ['minkowski', 'euclidean']\n",
    "    }\n",
    "    knn_grid = GridSearchCV(\n",
    "        KNeighborsRegressor(),\n",
    "        knn_params,\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=0\n",
    "    )\n",
    "    knn_grid.fit(X_train, y_train)\n",
    "    optimized_models.append(('knn', knn_grid.best_estimator_))\n",
    "    print(f\"          ‚úì Melhores params: {knn_grid.best_params_}\")\n",
    "    \n",
    "    # ===== ElasticNet (NOVO) =====\n",
    "    print(f\"        - ElasticNet...\")\n",
    "    en_params = {\n",
    "        'alpha': [0.1, 0.5, 1.0, 5.0, 10.0],\n",
    "        'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "        'max_iter': [5000, 10000]\n",
    "    }\n",
    "    en_grid = GridSearchCV(\n",
    "        ElasticNet(random_state=42),\n",
    "        en_params,\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=n_jobs,\n",
    "        verbose=0\n",
    "    )\n",
    "    en_grid.fit(X_train, y_train)\n",
    "    optimized_models.append(('elasticnet', en_grid.best_estimator_))\n",
    "    print(f\"          ‚úì Melhores params: {en_grid.best_params_}\")\n",
    "    \n",
    "    return optimized_models\n",
    "\n",
    "# ===== META-MODELO COM GRID SEARCH =====\n",
    "\n",
    "def get_optimized_meta_model(X_train, y_train, base_models):\n",
    "    \"\"\"\n",
    "    Otimiza o meta-modelo com Grid Search.\n",
    "    \"\"\"\n",
    "    print(f\"      üîç Otimizando meta-modelo...\")\n",
    "    \n",
    "    cv_splits = min(3, len(X_train) // 20)\n",
    "    tscv = TimeSeriesSplit(n_splits=cv_splits)\n",
    "    \n",
    "    # Testar Ridge e ElasticNet como meta-modelos\n",
    "    meta_params = [\n",
    "        {\n",
    "            'final_estimator': [Ridge()],\n",
    "            'final_estimator__alpha': [0.1, 1.0, 10.0, 50.0, 100.0]\n",
    "        },\n",
    "        {\n",
    "            'final_estimator': [ElasticNet(max_iter=10000)],\n",
    "            'final_estimator__alpha': [0.1, 1.0, 10.0],\n",
    "            'final_estimator__l1_ratio': [0.1, 0.5, 0.9]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    best_score = float('-inf')\n",
    "    best_meta = None\n",
    "    \n",
    "    for params in meta_params:\n",
    "        stacking = StackingRegressor(\n",
    "            estimators=base_models,\n",
    "            cv=cv_splits,\n",
    "            n_jobs=-1,\n",
    "            passthrough=True\n",
    "        )\n",
    "        \n",
    "        grid = GridSearchCV(\n",
    "            stacking,\n",
    "            params,\n",
    "            cv=tscv,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        grid.fit(X_train, y_train)\n",
    "        \n",
    "        if grid.best_score_ > best_score:\n",
    "            best_score = grid.best_score_\n",
    "            best_meta = grid.best_estimator_\n",
    "            best_params = grid.best_params_\n",
    "    \n",
    "    print(f\"        ‚úì Melhor meta-modelo: {best_params}\")\n",
    "    return best_meta\n",
    "\n",
    "# ========== AVALIA√á√ÉO STACKING COM GRID SEARCH E CV ==========\n",
    "\n",
    "def evaluate_stacking_cv(df_clean: pd.DataFrame, test_fraction: float, target_col: str = \"Vazao_BBR\") -> Dict:\n",
    "    \"\"\"\n",
    "    Avalia Stacking com Grid Search usando Time Series Split.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    print(f\"  [STACKING] Avaliando com test_fraction={test_fraction:.0%}\")\n",
    "    \n",
    "    n_splits = max(3, min(5, int(1 / test_fraction)))\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    \n",
    "    all_metrics = []\n",
    "    all_prediction_times = []\n",
    "    print(f\"    Cross-validation com {n_splits} splits\")\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(df_clean)):\n",
    "        desired_test_size = int(len(df_clean) * test_fraction)\n",
    "        if len(test_idx) > desired_test_size:\n",
    "            test_idx = test_idx[-desired_test_size:]\n",
    "\n",
    "        df_train = df_clean.iloc[train_idx].copy()\n",
    "        df_test = df_clean.iloc[test_idx].copy()\n",
    "\n",
    "        df_train = engineer_features_for_imputation(df_train, target_col)\n",
    "        df_test = engineer_features_for_imputation(df_test, target_col)\n",
    "\n",
    "        exclude_cols = {target_col, 'Data', 'mask_applied'}\n",
    "        feature_cols = [c for c in df_train.columns if c not in exclude_cols]\n",
    "\n",
    "        df_train = df_train.dropna(subset=feature_cols + [target_col])\n",
    "        df_test = df_test.dropna(subset=feature_cols + [target_col])\n",
    "\n",
    "        if df_train.empty or df_test.empty:\n",
    "            print(f\"      ‚ö†Ô∏è Fold {fold+1} ignorado (dados insuficientes).\")\n",
    "            continue\n",
    "\n",
    "        X_train, y_train = df_train[feature_cols].fillna(0).values, df_train[target_col].values\n",
    "        X_test, y_test = df_test[feature_cols].fillna(0).values, df_test[target_col].values\n",
    "\n",
    "        print(f\"      Fold {fold+1}: treino={len(X_train)}, teste={len(X_test)} ({len(X_test)/len(df_clean)*100:.1f}%)\")\n",
    "\n",
    "        try:\n",
    "            # Escalonamento\n",
    "            scaler_X = StandardScaler()\n",
    "            scaler_y = StandardScaler()\n",
    "\n",
    "            X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "            X_test_scaled = scaler_X.transform(X_test)\n",
    "            y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "\n",
    "            # ‚úÖ GRID SEARCH NOS MODELOS BASE\n",
    "            base_models = get_optimized_base_models(X_train_scaled, y_train_scaled, n_jobs=-1)\n",
    "\n",
    "            # ‚úÖ GRID SEARCH NO META-MODELO\n",
    "            stacking = get_optimized_meta_model(X_train_scaled, y_train_scaled, base_models)\n",
    "\n",
    "            # Medir tempo de predi√ß√£o\n",
    "            start_time = time.time()\n",
    "            y_pred_scaled = stacking.predict(X_test_scaled)\n",
    "            prediction_time = time.time() - start_time\n",
    "            \n",
    "            y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "            metrics = calculate_metrics(y_test, y_pred, prediction_time)\n",
    "\n",
    "            if metrics['rmse'] is not None:\n",
    "                all_metrics.append(metrics)\n",
    "                all_prediction_times.append(prediction_time)\n",
    "                print(f\"        ‚úÖ RMSE: {metrics['rmse']:.2f}M, R¬≤: {metrics['r2']:.4f}, \"\n",
    "                      f\"Tempo: {metrics['prediction_time_per_sample']:.4f}ms/amostra\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"        ‚ö†Ô∏è Erro no fold {fold+1}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # C√°lculo da m√©dia das m√©tricas\n",
    "    if all_metrics:\n",
    "        avg_metrics = {}\n",
    "        for metric in ['rmse', 'nrmse', 'r2', 'mape', 'prediction_time_per_sample']:\n",
    "            values = [m[metric] for m in all_metrics if m[metric] is not None]\n",
    "            avg_metrics[metric] = round(np.mean(values), 4) if values else None\n",
    "        \n",
    "        if all_prediction_times:\n",
    "            avg_metrics['total_prediction_time'] = round(np.mean(all_prediction_times), 4)\n",
    "            avg_metrics['total_samples_predicted'] = sum(len(m) for m in [\n",
    "                df_clean.iloc[test_idx] for _, test_idx in tscv.split(df_clean)\n",
    "            ] if len(test_idx) <= int(len(df_clean) * test_fraction))\n",
    "\n",
    "        print(f\"    üìä M√©dia - RMSE: {avg_metrics['rmse']:.2f}M, R¬≤: {avg_metrics['r2']:.4f}, \"\n",
    "              f\"Tempo: {avg_metrics['prediction_time_per_sample']:.4f}ms/amostra\")\n",
    "        return avg_metrics\n",
    "    else:\n",
    "        print(f\"    ‚ùå Nenhuma m√©trica v√°lida calculada\")\n",
    "        return {\"rmse\": None, \"nrmse\": None, \"r2\": None, \"mape\": None, \"prediction_time_per_sample\": None}\n",
    "\n",
    "# ========== PIPELINE COMPLETO DE AVALIA√á√ÉO ==========\n",
    "\n",
    "def evaluate_file(file_path: Path, missing_fractions: List[float]) -> Dict:\n",
    "    \"\"\"\n",
    "    Pipeline completo de avalia√ß√£o para um arquivo CSV.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"[AVALIANDO] {file_path.name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    source = file_path.stem.replace(\"_merged\", \"\").replace(\"_largest_subseries\", \"\")\n",
    "    target_col = \"Vazao_BBR\"\n",
    "    \n",
    "    df_clean = get_clean_data(df, target_col)\n",
    "    \n",
    "    if len(df_clean) < 20:\n",
    "        print(f\"  ‚ö†Ô∏è Dados insuficientes: apenas {len(df_clean)} amostras\")\n",
    "        return None\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for frac in missing_fractions:\n",
    "        print(f\"\\n  {'‚îÄ'*60}\")\n",
    "        print(f\"  FRA√á√ÉO DE MISSING: {frac:.0%}\")\n",
    "        print(f\"  {'‚îÄ'*60}\")\n",
    "        \n",
    "        baseline_results = evaluate_baselines(df_clean, frac, target_col)\n",
    "        stacking_results = evaluate_stacking_cv(df_clean, frac, target_col)\n",
    "        \n",
    "        results[str(frac)] = {\n",
    "            \"baseline\": baseline_results,\n",
    "            \"stacking\": {\n",
    "                \"mean\": {\n",
    "                    \"StackingRegressor\": stacking_results\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        \"source\": source,\n",
    "        \"results\": results,\n",
    "        \"n_samples\": len(df_clean)\n",
    "    }\n",
    "\n",
    "# ========== AN√ÅLISE DE DESEMPENHO ==========\n",
    "\n",
    "def analyze_stacking_performance(results: Dict) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analisa se o stacking foi melhor que as baselines.\n",
    "    \"\"\"\n",
    "    stacking_wins = 0\n",
    "    total_comparisons = 0\n",
    "    stacking_rmse_list = []\n",
    "    best_baseline_rmse_list = []\n",
    "    prediction_times = []\n",
    "    \n",
    "    for frac, data in results.items():\n",
    "        baseline_data = data.get(\"baseline\", {})\n",
    "        stacking_data = data.get(\"stacking\", {}).get(\"mean\", {}).get(\"StackingRegressor\", {})\n",
    "        \n",
    "        if not baseline_data or not stacking_data:\n",
    "            continue\n",
    "        \n",
    "        stacking_rmse = stacking_data.get(\"rmse\")\n",
    "        if stacking_rmse is None:\n",
    "            continue\n",
    "        \n",
    "        pred_time = stacking_data.get(\"prediction_time_per_sample\")\n",
    "        if pred_time is not None:\n",
    "            prediction_times.append(pred_time)\n",
    "        \n",
    "        baseline_rmses = [\n",
    "            metrics[\"rmse\"] for metrics in baseline_data.values() \n",
    "            if metrics.get(\"rmse\") is not None\n",
    "        ]\n",
    "        \n",
    "        if not baseline_rmses:\n",
    "            continue\n",
    "        \n",
    "        best_baseline_rmse = min(baseline_rmses)\n",
    "        \n",
    "        total_comparisons += 1\n",
    "        stacking_rmse_list.append(stacking_rmse)\n",
    "        best_baseline_rmse_list.append(best_baseline_rmse)\n",
    "        \n",
    "        if stacking_rmse < best_baseline_rmse:\n",
    "            stacking_wins += 1\n",
    "    \n",
    "    if total_comparisons == 0:\n",
    "        return {\n",
    "            \"should_impute\": False,\n",
    "            \"win_rate\": 0.0,\n",
    "            \"avg_improvement\": 0.0,\n",
    "            \"total_comparisons\": 0,\n",
    "            \"avg_prediction_time_per_sample\": None\n",
    "        }\n",
    "    \n",
    "    win_rate = stacking_wins / total_comparisons\n",
    "    avg_stacking_rmse = np.mean(stacking_rmse_list)\n",
    "    avg_best_baseline_rmse = np.mean(best_baseline_rmse_list)\n",
    "    avg_improvement = ((avg_best_baseline_rmse - avg_stacking_rmse) / avg_best_baseline_rmse) * 100\n",
    "    \n",
    "    avg_pred_time = round(np.mean(prediction_times), 4) if prediction_times else None\n",
    "    \n",
    "    return {\n",
    "        \"should_impute\": win_rate >= 0.5,\n",
    "        \"win_rate\": win_rate,\n",
    "        \"avg_improvement\": avg_improvement,\n",
    "        \"total_comparisons\": total_comparisons,\n",
    "        \"stacking_wins\": stacking_wins,\n",
    "        \"avg_stacking_rmse\": avg_stacking_rmse,\n",
    "        \"avg_baseline_rmse\": avg_best_baseline_rmse,\n",
    "        \"avg_prediction_time_per_sample\": avg_pred_time\n",
    "    }\n",
    "\n",
    "# ========== IMPUTA√á√ÉO DE DADOS ==========\n",
    "\n",
    "def impute_with_baselines(df: pd.DataFrame, target_col: str, output_dir: Path, source: str):\n",
    "    \"\"\"\n",
    "    Imputa valores -1 usando m√©todos baseline.\n",
    "    \"\"\"\n",
    "    mask_missing = df[target_col] == -1\n",
    "    n_missing = mask_missing.sum()\n",
    "    \n",
    "    if n_missing == 0:\n",
    "        return\n",
    "    \n",
    "    print(f\"    [BASELINES] Imputando {n_missing} valores...\")\n",
    "    \n",
    "    df_clean = df[df[target_col] != -1].copy()\n",
    "    cols_to_save = [\"Data\", \"Atraso(ms)\", \"Hop_count\", \"Bottleneck\", target_col, \"is_imputed\"]\n",
    "    \n",
    "    # M√âDIA\n",
    "    try:\n",
    "        df_mean = df.copy()\n",
    "        df_mean['is_imputed'] = 0\n",
    "        df_mean.loc[mask_missing, 'is_imputed'] = 1\n",
    "        df_mean.loc[mask_missing, target_col] = df_clean[target_col].mean()\n",
    "        df_mean[cols_to_save].to_csv(output_dir / f\"{source}_baseline_mean.csv\", index=False)\n",
    "        print(f\"      ‚úì Mean: {output_dir / f'{source}_baseline_mean.csv'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"      ‚úó Mean falhou: {e}\")\n",
    "    \n",
    "    # MEDIANA\n",
    "    try:\n",
    "        df_median = df.copy()\n",
    "        df_median['is_imputed'] = 0\n",
    "        df_median.loc[mask_missing, 'is_imputed'] = 1\n",
    "        df_median.loc[mask_missing, target_col] = df_clean[target_col].median()\n",
    "        df_median[cols_to_save].to_csv(output_dir / f\"{source}_baseline_median.csv\", index=False)\n",
    "        print(f\"      ‚úì Median: {output_dir / f'{source}_baseline_median.csv'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"      ‚úó Median falhou: {e}\")\n",
    "    \n",
    "    # KNN\n",
    "    try:\n",
    "        df_knn = df.copy()\n",
    "        df_knn['is_imputed'] = 0\n",
    "        df_knn.loc[mask_missing, 'is_imputed'] = 1\n",
    "        imputer = KNNImputer(n_neighbors=min(5, len(df_clean) // 2))\n",
    "        df_knn[target_col] = imputer.fit_transform(df[[target_col]].replace(-1, np.nan)).ravel()\n",
    "        df_knn[cols_to_save].to_csv(output_dir / f\"{source}_baseline_knn.csv\", index=False)\n",
    "        print(f\"      ‚úì KNN: {output_dir / f'{source}_baseline_knn.csv'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"      ‚úó KNN falhou: {e}\")\n",
    "    \n",
    "    # FORWARD FILL\n",
    "    try:\n",
    "        df_ffill = df.copy()\n",
    "        df_ffill['is_imputed'] = 0\n",
    "        df_ffill.loc[mask_missing, 'is_imputed'] = 1\n",
    "        df_ffill[target_col] = df_ffill[target_col].replace(-1, np.nan).ffill().bfill()\n",
    "        df_ffill[cols_to_save].to_csv(output_dir / f\"{source}_baseline_ffill.csv\", index=False)\n",
    "        print(f\"      ‚úì ForwardFill: {output_dir / f'{source}_baseline_ffill.csv'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"      ‚úó ForwardFill falhou: {e}\")\n",
    "    \n",
    "    # BACKWARD FILL\n",
    "    try:\n",
    "        df_bfill = df.copy()\n",
    "        df_bfill['is_imputed'] = 0\n",
    "        df_bfill.loc[mask_missing, 'is_imputed'] = 1\n",
    "        df_bfill[target_col] = df_bfill[target_col].replace(-1, np.nan).bfill().ffill()\n",
    "        df_bfill[cols_to_save].to_csv(output_dir / f\"{source}_baseline_bfill.csv\", index=False)\n",
    "        print(f\"      ‚úì BackwardFill: {output_dir / f'{source}_baseline_bfill.csv'}\")\n",
    "    except Exception as e:\n",
    "        print(f\"      ‚úó BackwardFill falhou: {e}\")\n",
    "\n",
    "def impute_with_stacking(df: pd.DataFrame, target_col: str, output_dir: Path, source: str):\n",
    "    \"\"\"\n",
    "    Imputa valores -1 usando Stacking Regressor OTIMIZADO com Grid Search.\n",
    "    \"\"\"\n",
    "    mask_missing = df[target_col] == -1\n",
    "    n_missing = mask_missing.sum()\n",
    "    \n",
    "    if n_missing == 0:\n",
    "        return\n",
    "    \n",
    "    print(f\"    [STACKING OTIMIZADO] Imputando {n_missing} valores...\")\n",
    "    \n",
    "    try:\n",
    "        df_clean = df[df[target_col] != -1].copy()\n",
    "        \n",
    "        if len(df_clean) < 10:\n",
    "            print(f\"      ‚úó Dados insuficientes para treinar stacking\")\n",
    "            return\n",
    "        \n",
    "        df_clean_feat = engineer_features_for_imputation(df_clean, target_col)\n",
    "        \n",
    "        exclude_cols = {target_col, 'Data', 'mask_applied'}\n",
    "        feature_cols = [c for c in df_clean_feat.columns \n",
    "                       if c not in exclude_cols and pd.api.types.is_numeric_dtype(df_clean_feat[c])]\n",
    "        \n",
    "        X_train = df_clean_feat[feature_cols].fillna(0).values\n",
    "        y_train = df_clean_feat[target_col].values\n",
    "        \n",
    "        print(f\"      Treinando com {len(X_train)} amostras e {len(feature_cols)} features\")\n",
    "        \n",
    "        # Escalonamento\n",
    "        scaler_X = StandardScaler()\n",
    "        scaler_y = StandardScaler()\n",
    "        \n",
    "        X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "        y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "        \n",
    "        # ‚úÖ GRID SEARCH NOS MODELOS BASE\n",
    "        print(f\"      üîç Otimizando modelos base...\")\n",
    "        base_models = get_optimized_base_models(X_train_scaled, y_train_scaled, n_jobs=-1)\n",
    "        \n",
    "        # ‚úÖ GRID SEARCH NO META-MODELO\n",
    "        print(f\"      üîç Otimizando meta-modelo...\")\n",
    "        stacking = get_optimized_meta_model(X_train_scaled, y_train_scaled, base_models)\n",
    "        \n",
    "        # IMPUTAR LINHA POR LINHA\n",
    "        df_imputed = df.copy()\n",
    "        df_imputed['is_imputed'] = 0\n",
    "        \n",
    "        missing_indices = df[mask_missing].index.tolist()\n",
    "        imputed_values = []\n",
    "        \n",
    "        print(f\"      Imputando {len(missing_indices)} valores...\")\n",
    "        \n",
    "        for idx in missing_indices:\n",
    "            df_temp = df_imputed.iloc[:idx+1].copy()\n",
    "            \n",
    "            if df_temp.loc[idx, target_col] == -1:\n",
    "                valid_values = df_temp[df_temp[target_col] != -1][target_col]\n",
    "                if len(valid_values) > 0:\n",
    "                    df_temp.loc[idx, target_col] = valid_values.median()\n",
    "                else:\n",
    "                    df_temp.loc[idx, target_col] = df_clean[target_col].median()\n",
    "            \n",
    "            df_temp_feat = engineer_features_for_imputation(df_temp, target_col)\n",
    "            X_pred = df_temp_feat.iloc[-1:][feature_cols].fillna(0).values\n",
    "            X_pred_scaled = scaler_X.transform(X_pred)\n",
    "            \n",
    "            y_pred_scaled = stacking.predict(X_pred_scaled)\n",
    "            y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()[0]\n",
    "            \n",
    "            df_imputed.loc[idx, target_col] = y_pred\n",
    "            df_imputed.loc[idx, 'is_imputed'] = 1\n",
    "            imputed_values.append(y_pred)\n",
    "        \n",
    "        # Salvar resultado\n",
    "        cols_to_save = [\"Data\", \"Atraso(ms)\", \"Hop_count\", \"Bottleneck\", target_col, \"is_imputed\"]\n",
    "        output_file = output_dir / f\"{source}_stacking_optimized.csv\"\n",
    "        df_imputed[cols_to_save].to_csv(output_file, index=False)\n",
    "        \n",
    "        # Estat√≠sticas\n",
    "        imputed_array = np.array(imputed_values)\n",
    "        print(f\"      ‚úì Stacking Otimizado: {output_file}\")\n",
    "        print(f\"      ‚úì Valores imputados: {n_missing}\")\n",
    "        print(f\"      ‚úì M√©dia: {np.mean(imputed_array)/1e6:.2f}M\")\n",
    "        print(f\"      ‚úì Desvio: {np.std(imputed_array)/1e6:.2f}M\")\n",
    "        print(f\"      ‚úì Min: {np.min(imputed_array)/1e6:.2f}M, Max: {np.max(imputed_array)/1e6:.2f}M\")\n",
    "        print(f\"      ‚úì Valores √∫nicos: {len(np.unique(imputed_array))}/{n_missing}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      ‚úó Stacking falhou: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "def intelligent_imputation(file_path: Path, results: Dict, output_dir: Path):\n",
    "    \"\"\"\n",
    "    Decide se deve imputar baseado nos resultados da avalia√ß√£o.\n",
    "    \"\"\"\n",
    "    source = results[\"source\"]\n",
    "    analysis = analyze_stacking_performance(results[\"results\"])\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"[AN√ÅLISE DE DESEMPENHO] {source}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  Vit√≥rias do Stacking: {analysis['stacking_wins']}/{analysis['total_comparisons']} ({analysis['win_rate']*100:.1f}%)\")\n",
    "    print(f\"  RMSE M√©dio Stacking: {analysis['avg_stacking_rmse']:.2f}M\")\n",
    "    print(f\"  RMSE M√©dio Melhor Baseline: {analysis['avg_baseline_rmse']:.2f}M\")\n",
    "    print(f\"  Melhoria M√©dia: {analysis['avg_improvement']:.2f}%\")\n",
    "    \n",
    "    if analysis.get('avg_prediction_time_per_sample') is not None:\n",
    "        print(f\"  ‚è±Ô∏è  Tempo M√©dio de Predi√ß√£o: {analysis['avg_prediction_time_per_sample']:.4f}ms/amostra\")\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    target_col = \"Vazao_BBR\"\n",
    "    n_missing = (df[target_col] == -1).sum()\n",
    "    \n",
    "    if n_missing == 0:\n",
    "        print(f\"  ‚ÑπÔ∏è  Nenhum valor faltante para imputar\")\n",
    "        return\n",
    "    \n",
    "    print(f\"  üìä {n_missing} valores faltantes encontrados\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if analysis['should_impute']:\n",
    "        print(f\"  ‚úÖ DECIS√ÉO: Stacking melhor - IMPUTANDO COM TODOS OS M√âTODOS\")\n",
    "        print(f\"\\n  {'‚îÄ'*60}\")\n",
    "        \n",
    "        impute_with_baselines(df, target_col, output_dir, source)\n",
    "        impute_with_stacking(df, target_col, output_dir, source)\n",
    "        \n",
    "        print(f\"  {'‚îÄ'*60}\")\n",
    "        print(f\"  ‚úÖ Imputa√ß√£o conclu√≠da para {source}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"  ‚ùå DECIS√ÉO: Stacking N√ÉO foi melhor - APENAS BASELINES\")\n",
    "        print(f\"\\n  {'‚îÄ'*60}\")\n",
    "        \n",
    "        impute_with_baselines(df, target_col, output_dir, source)\n",
    "        \n",
    "        print(f\"  {'‚îÄ'*60}\")\n",
    "        print(f\"  ‚ö†Ô∏è  Stacking n√£o aplicado para {source}\")\n",
    "\n",
    "# ========== EXECU√á√ÉO PRINCIPAL ==========\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Executa avalia√ß√£o completa e imputa√ß√£o inteligente com Grid Search.\n",
    "    \"\"\"\n",
    "    data_path = Path(\"../../datasets/multivariada-post-process\")\n",
    "    results_path = Path(\"../../results\")\n",
    "    imputed_path = Path(\"../../datasets/multivariada-imputed-optimized\")\n",
    "    \n",
    "    results_path.mkdir(exist_ok=True, parents=True)\n",
    "    imputed_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    csv_files = list(data_path.glob(\"*_merged.csv\"))\n",
    "    missing_fractions = [0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PIPELINE OTIMIZADO COM GRID SEARCH\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Arquivos encontrados: {len(csv_files)}\")\n",
    "    print(f\"Fra√ß√µes de missing: {missing_fractions}\")\n",
    "    print(f\"Pasta de resultados: {results_path}\")\n",
    "    print(f\"Pasta de dados imputados: {imputed_path}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    all_results = {}\n",
    "    summary = {\n",
    "        \"total_files\": len(csv_files),\n",
    "        \"processed\": 0,\n",
    "        \"stacking_used\": 0,\n",
    "        \"baseline_only\": 0,\n",
    "        \"failed\": 0\n",
    "    }\n",
    "    \n",
    "    for i, file_path in enumerate(csv_files, 1):\n",
    "        print(f\"\\n{'#'*80}\")\n",
    "        print(f\"[{i}/{len(csv_files)}] Processando: {file_path.name}\")\n",
    "        print(f\"{'#'*80}\")\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"FASE 1: AVALIA√á√ÉO COM GRID SEARCH\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            result = evaluate_file(file_path, missing_fractions)\n",
    "            \n",
    "            if result is None:\n",
    "                print(f\"  ‚ö†Ô∏è Arquivo ignorado (dados insuficientes)\")\n",
    "                summary[\"failed\"] += 1\n",
    "                continue\n",
    "            \n",
    "            source = result[\"source\"]\n",
    "            all_results[source] = result[\"results\"]\n",
    "            \n",
    "            evaluation_file = results_path / \"metrics_summary_optimized.json\"\n",
    "            with open(evaluation_file, 'w') as f:\n",
    "                json.dump(all_results, f, indent=4)\n",
    "            \n",
    "            print(f\"\\n  üíæ Avalia√ß√£o salva em: {evaluation_file}\")\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"FASE 2: IMPUTA√á√ÉO INTELIGENTE\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            intelligent_imputation(file_path, result, imputed_path)\n",
    "            \n",
    "            analysis = analyze_stacking_performance(result[\"results\"])\n",
    "            summary[\"processed\"] += 1\n",
    "            if analysis['should_impute']:\n",
    "                summary[\"stacking_used\"] += 1\n",
    "            else:\n",
    "                summary[\"baseline_only\"] += 1\n",
    "            \n",
    "            print(f\"\\n  ‚úÖ Conclu√≠do: {source}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n  ‚ùå Erro processando {file_path.name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            summary[\"failed\"] += 1\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"RELAT√ìRIO FINAL\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total de arquivos: {summary['total_files']}\")\n",
    "    print(f\"Processados com sucesso: {summary['processed']}\")\n",
    "    print(f\"  ‚îú‚îÄ Stacking usado: {summary['stacking_used']} ({summary['stacking_used']/max(1,summary['processed'])*100:.1f}%)\")\n",
    "    print(f\"  ‚îî‚îÄ Apenas baselines: {summary['baseline_only']} ({summary['baseline_only']/max(1,summary['processed'])*100:.1f}%)\")\n",
    "    print(f\"Falhas: {summary['failed']}\")\n",
    "    print(f\"\\nüìÅ Resultados salvos em:\")\n",
    "    print(f\"  ‚îú‚îÄ Avalia√ß√£o: {results_path / 'metrics_summary_optimized.json'}\")\n",
    "    print(f\"  ‚îî‚îÄ Dados imputados: {imputed_path}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    summary_file = results_path / \"imputation_summary_optimized.json\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(summary, f, indent=4)\n",
    "    \n",
    "    print(f\"üìä Sum√°rio salvo em: {summary_file}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
